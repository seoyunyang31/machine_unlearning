# Training Scripts

This directory contains the Python scripts for training the Neural Collaborative Filtering (NCF) model.

## Scripts

### `train_baseline.py`

This is the primary, production-grade script used to train the initial "teacher" model. It is designed for scientific rigor and reproducibility.

**Key Features:**
- **Reproducibility:** Sets global random seeds to ensure consistent results.
- **On-the-Fly Negative Sampling:** Uses the custom `NCFDataset` to efficiently generate negative samples during training, requiring only a dataset of positive interactions.
- **Metric-Driven Checkpointing:** Tracks the NDCG@10 score on the validation set after each epoch and saves only the best-performing model.
- **Early Stopping:** Prevents overfitting by automatically stopping the training process if the validation NDCG@10 does not improve for a set number of epochs (patience).
- **Detailed Logging:** Logs all configuration parameters, training progress, and evaluation metrics to both the console and `training.log`.

**Usage:**
This script is run without command-line arguments. Configuration is handled by the `config` dictionary at the top of the file.

```bash
python src/training/train_baseline.py
```

**Output:**
- The best model is saved to `models/ncf_best.pth`.
- A detailed log file is generated at `training.log`.

---

### `train_template.py`

This is a simpler script intended to be used as a **template** for other training tasks, such as fine-tuning or unlearning experiments.

**Key Differences & Considerations:**
- **Data Format:** This script **does not use on-the-fly negative sampling**. It expects the `DataLoader` to provide batches containing `(user, item, label)`. This means it is **not** directly compatible for training with the `train_dataset.pt` generated by `prepare_dataset.py`, as that file only contains positive interactions. It is intended for use with a dataset that has already been prepared with both positive and negative samples.
- **Configuration:** Uses command-line arguments. Because all arguments have default values, the script can be run without any flags.
- **Evaluation:** It has been updated to use the same standardized `evaluate_1_vs_99` function as `train_baseline.py` to ensure all results are comparable.

**Usage:**
You can run the script without any flags to use the default hyperparameters.

```bash
python src/training/train_template.py
```

You can also override the defaults by providing command-line arguments.

```bash
# Example of overriding defaults
python src/training/train_template.py --epochs 15 --lr 0.002
```

**Default Hyperparameters:**
- `epochs`: 20
- `batch_size`: 256
- `embedding_dim`: 8
- `lr`: 0.001
- `dropout`: 0.1
- `hidden_layers`: [64, 32, 16]
- `top_k` (for evaluation): 10
